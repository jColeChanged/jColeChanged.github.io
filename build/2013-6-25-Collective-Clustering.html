<html><head><meta charset="UTF-8" /><title>Clustering</title><meta content="Toto document" name="description" /><meta content="" name="keywords" /><meta content="width=device-width, initial-scale=1" name="viewport" /><style>/* some style */

@font-face {
    font-family: "LatinModernRomanWOFF";
    /*src: url('https://www.albany.edu/~hammond/webfonts/lmroman12-regular.woff');*/
    src: url('http://ozviz.io/fonts/lmroman12-regular.woff');
}

/* for math fonts? */
/*@font-face {*/
    /*font-family: "LatinModernMathWOFF";*/
    /*src: url('https://www.albany.edu/~hammond/webfonts/latinmodern-math.woff');*/
    /*font-weight: bold;*/
/*}*/


body {
  padding: 30px;
  line-height: 1.7;
  font-family: 'Open Sans', sans-serif;
}

div#app {
  margin: 0;
  padding: 0;
  font-family: 'Open Sans', sans-serif;
}

blockquote {
  font-style: italic;
  color: #606060;
}

h1, h2, h3, h4 {
  font-family: "LatinModernRomanWOFF", serif;
  line-height: 1.4;
  /*font-family: 'Open Sans', sans-serif;*/
}

.fancy {
  font-family: "LatinModernRomanWOFF", serif;
}
.plain {
  font-family: 'Open Sans', sans-serif;
}

h1 {
  font-size: 50px;
  margin-top: 40px;
  margin-bottom: 20px;
}

h2{
  font-size: 30px;
  margin-top: 50px;
  margin-bottom: 15px;
}

h3 {
  font-size: 23px;
  margin-top: 40px;
  margin-bottom: 13px;
}

h4 {font-size: 18px;}

code {
  line-height: 1.5;
  font-size: 14.5px;
  color: #606060;
  margin-right: 4px;
  margin-left: 4px;
}

pre {
  line-height: 1.5;
  font-size: 14.5px;
  color: #606060;
}

a {
  color: darkblue;
}

div.vega-embed div.chart-wrapper {
  height: auto;
}


</style><link href="http://ozviz.io/fonts/lmroman12-regular.woff" rel="stylesheet" /><link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" /></head><body><div><p>Clustering is a bit like classification in that data points needs to be  categorized, but whereas classification is a supervised learning task in  which labels that help to guide classification, clustering is an unsupervised  learning task in which labels must be discovered.</p><p>When I was first introduced to code which could be used to cluster it was  in Tony Seagan's book <a href="book">Programming Collective Intelligence</a>. Following his  writing, I implemented hierachical clustering, k means clustering, and  multidimensional scaling.</p><p>As I've learned more about clustering and where it is used I've realized  that clustering is an essential tool to thinking well not just for AI, but  also for humans. We cluster so often and so naturally as humans that we  often don't notice just how many categories we have created for ourselves.  We cluster books by topic, and by genre. We cluster buildings by size. We  cluster sentences according to the logical structure. We cluster the various  domains of life. We even cluster situations about which we know nothing:  we distinguish between known knowns, known unknowns, and unknown unknowns. </p><p></p><div class="p"> One of the reasons clustering is essential is the world is extremely  complicated. It is necessary to simplify reality in order to be able to  think about it a time and space effecient manner. Without some tricks  that simplify or relax the problem of understanding a messy reality  the problem is quite literally not solvable, because it isn't possible to  fit the necessary computation into the limited time available to make that  computation.<label class="margin-toggle sidenote-number" for="sn-biases"></label>   <input <pre="" type="checkbox" /><code>    id="sn-biases"
    class="margin-toggle"/> </code>  <div class="sidenote"><pre><code><p>
  For related reasons, some cognitive biases people refer to as failings are 
  actually deeply rational. The mistake people make in talking about this is 
  that they confuse the problems they are solving with the problems the mind 
  is solving. They solve the problem in a non-realtime context, but the mind 
  is solving it in a realtime context. They are solving the problem in a 
  manner that applies attention to the important details. The mind doesn't 
  start with details. It starts with a sensory feed that must be converted 
  and transformed into something meaningful. That sensory feed is so large 
  that it can't even process all of it. It has to guess at what the important 
  bits are if it is going to have a hope of keeping up.
</p> </code></pre>  </div> </div><p></p><h1>Hierarchical Clustering</h1><p>[Hierarchical clustering][1] starts out by saying that every element that you are looking at is its own distinct cluster. Then it tries to find the two clusters that are closest to each other. Once it finds those clusters it merges them. In some ways things are arbitrary. I can choose to look at any number of clusters I want. If I only want to look at three clusters I can stop running things when I get down to three clusters. However, I can also make a [dendrogram][2] if I make sure to incorporate the tree-like structure of the clustering process into my code.</p><p>While working through the last chapter I created a few dendrogram. The dendrogram presented below is the result of clustering 100 of the most popular blogs by the words used in their RSS feed. Of particular interest is that Google search related blogs tended to come out near each other.</p><p>![Dendrogram of Blog Clustering by Word Use][3]</p><p>A far more interesting dendrogram that I produced show what desires tend to be grouped together. A now dead site named zebo.com used to collect information about what people wanted. Using the data that they had made available I made a dendrogram of what things people tended to like. I found pursuing this graphic to be quite enjoyable.</p><p>![Dendrogram of Desires by Shared Desires][4]</p><h1>k-means Clustering</h1><p>Earlier I said that hierarchical clustering compared everything to everything else in order to figure out what two clusters were the closest to each other. As you can imagine this has terrible performance. O(n<sup>2)</sup> to be more precise. Memoization can be used to make things a bit faster then that in practice, but the algorithm still doesn't have the most wonderful scaling properties even with that. That is where K Means clustering comes in.</p><p>In [k-means clustering][5] you basically establish some dummy clusters called centroids. You create k of them. Then you find the closest centroid to a cluster. You lose the ability to make a dendrogram, things becomes even more arbitrary since the centroid selection is pretty random, and in exchange you gain an O(nk) search for the nearest element.</p><h1>Multidimensional Scaling</h1><p>In multidimensional scaling you keep jiggering things by comparing them to a distance function on two variables and seeing how much that differs from a distant function on all the variables. This lets you reduce the number of variables while maintaining some sense of correlation. It also lets you plot things that have multiple dimensions in two dimensional space.</p><p>![Blogs By Scaled Down Word Count Feature Vector][6]</p><h1>Things I Built</h1><ul><li>I visualized delicious data via a dendrogram that I computed.</li><li>I implemented and used Euclidean and Manhattan distance algorithms for clustering.</li><li>I experimented with different values of <code>k</code> when using k-means clustering and  saw how doing so changed the clustering error.</li><li>I implemented multidimensional scaling for both one dimension and three dimensions.</li></ul><p>[1]: https://en.wikipedia.org/wiki/Hierarchical_clustering [2]: http://en.wikipedia.org/wiki/Dendrogram [3]: /img/collective-cluster/dend1.jpg "Dendrogram of Blog Clustering by Word Use" [4]: /img/collective-cluster/dend2.jpg "Dendrogram of Desires by Shared Desires" [5]: https://en.wikipedia.org/wiki/K-means_clustering [6]: /img/collective-cluster/scaled.jpg "Blogs Scaled Down" [book]: https://www.amazon.com/gp/product/0596529325/ref=as<i>li</i>tl?ie=UTF8&tag=joshuacoles-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0596529325&linkId=6e48c22fa422df9f35994c4acd00ac10</p></div></body></html>