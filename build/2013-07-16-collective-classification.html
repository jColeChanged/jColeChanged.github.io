<html><head><meta charset="UTF-8" /><title>Collective Classification</title><meta content="Toto document" name="description" /><meta content="" name="keywords" /><meta content="width=device-width, initial-scale=1" name="viewport" /><style>/* some style */

@font-face {
    font-family: "LatinModernRomanWOFF";
    /*src: url('https://www.albany.edu/~hammond/webfonts/lmroman12-regular.woff');*/
    src: url('http://ozviz.io/fonts/lmroman12-regular.woff');
}

/* for math fonts? */
/*@font-face {*/
    /*font-family: "LatinModernMathWOFF";*/
    /*src: url('https://www.albany.edu/~hammond/webfonts/latinmodern-math.woff');*/
    /*font-weight: bold;*/
/*}*/


body {
  padding: 30px;
  line-height: 1.7;
  font-family: 'Open Sans', sans-serif;
}

div#app {
  margin: 0;
  padding: 0;
  font-family: 'Open Sans', sans-serif;
}

blockquote {
  font-style: italic;
  color: #606060;
}

h1, h2, h3, h4 {
  font-family: "LatinModernRomanWOFF", serif;
  line-height: 1.4;
  /*font-family: 'Open Sans', sans-serif;*/
}

.fancy {
  font-family: "LatinModernRomanWOFF", serif;
}
.plain {
  font-family: 'Open Sans', sans-serif;
}

h1 {
  font-size: 50px;
  margin-top: 40px;
  margin-bottom: 20px;
}

h2{
  font-size: 30px;
  margin-top: 50px;
  margin-bottom: 15px;
}

h3 {
  font-size: 23px;
  margin-top: 40px;
  margin-bottom: 13px;
}

h4 {font-size: 18px;}

code {
  line-height: 1.5;
  font-size: 14.5px;
  color: #606060;
  margin-right: 4px;
  margin-left: 4px;
}

pre {
  line-height: 1.5;
  font-size: 14.5px;
  color: #606060;
}

a {
  color: darkblue;
}

div.vega-embed div.chart-wrapper {
  height: auto;
}


</style><link href="http://ozviz.io/fonts/lmroman12-regular.woff" rel="stylesheet" /><link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" /></head><body><div><p>This is another blog post in my series of blogs posts covering my reading of <a href="book">Programming Collective Intelligence</a>. A lot of the group members have had full-plates as of late. Some are working on start-ups, others are working through other books, and some are doing additional study groups. As a result the group has decided to cut back to one chapter per week. I'm finding that pace to be quite comfortable.</p><p>This weeks chapter was on document classification. I've done something a bit like it before when as a part of my Google internship application I wrote a Na√Øve Bayes Classifier minus the classification which ranked job applicants according to their chances of being hired. So in a way this chapter was a bit of review.</p><p>However, unlike what I wrote in the past this program took an extra step after getting the probabilities. It didn't try to rank things. It tried to classify them. So it might try to put things into two bins: spam and not spam.</p><p>This actually changed the problem a little bit. It turns out there is more going on when you are classifying then mapping a curried probability of an item over every category and taking the category which had the highest probability. You can consider getting fancy and give preferential treatment to some categories. For example in the case of classifying spam, you might give special treatment to the not-spam category in order to ensure that users don't miss their mail.</p><p>[book]: https://www.amazon.com/gp/product/0596529325/ref=as<i>li</i>tl?ie=UTF8&tag=joshuacoles-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0596529325&linkId=6e48c22fa422df9f35994c4acd00ac10</p></div></body></html>